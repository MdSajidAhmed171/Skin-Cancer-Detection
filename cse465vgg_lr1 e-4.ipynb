{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CSE499B_best_so_far-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a98a14720564657a7367f6c245fa14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_603a96e19fdf4de7910d15ea456324f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ba3460dc3694c6091eb09ee73bce284",
              "IPY_MODEL_a5150a77ac5a4c3aadf01d6d8a590be5"
            ]
          }
        },
        "603a96e19fdf4de7910d15ea456324f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ba3460dc3694c6091eb09ee73bce284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_41fbbabe6770400e8b4a5d667d399c37",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 531503671,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 531503671,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30bb28c85e6d456a9256c0b55517312a"
          }
        },
        "a5150a77ac5a4c3aadf01d6d8a590be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e79f27608ca84c79b75a41ad255e36b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 507M/507M [7:27:32&lt;00:00, 19.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2907c43af34c4836937d3bcb3316d4f5"
          }
        },
        "41fbbabe6770400e8b4a5d667d399c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30bb28c85e6d456a9256c0b55517312a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e79f27608ca84c79b75a41ad255e36b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2907c43af34c4836937d3bcb3316d4f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6YDfK_IZmiY"
      },
      "source": [
        "**IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUj4LOknyJzD",
        "outputId": "4619132d-d15a-4bf7-ff9d-66bdabd1ebbd"
      },
      "source": [
        "%matplotlib inline\n",
        "# python libraties\n",
        "import os, cv2,itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "from torch import optim,nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import models,transforms\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "torch.cuda.manual_seed(10)\n",
        "\n",
        "print(os.listdir(\"/content/drive/MyDrive/input499\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['HAM10000_metadata.csv', 'hmnist_28_28_L.csv', 'hmnist_28_28_RGB.csv', 'hmnist_8_8_RGB.csv', 'hmnist_8_8_L.csv', 'HAM10000_images_part_1', 'HAM10000_images_part_2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh90iHEtaDJ2",
        "outputId": "96428574-c40d-4c29-b2d9-5c9e50a55961"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lfWWyG8ZxcW"
      },
      "source": [
        "**Data analysis and preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVrJGWevZy9F"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/input499'\n",
        "all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf4RuHBceJd8"
      },
      "source": [
        "def compute_img_mean_std(image_paths):\n",
        "    \"\"\"\n",
        "        computing the mean and std of three channel on the whole dataset,\n",
        "        first we should normalize the image from 0-255 to 0-1\n",
        "    \"\"\"\n",
        "\n",
        "    img_h, img_w = 224, 224\n",
        "    imgs = []\n",
        "    means, stdevs = [], []\n",
        "\n",
        "    for i in tqdm(range(len(image_paths))):\n",
        "        img = cv2.imread(image_paths[i])\n",
        "        img = cv2.resize(img, (img_h, img_w))\n",
        "        imgs.append(img)\n",
        "\n",
        "    imgs = np.stack(imgs, axis=3)\n",
        "    print(imgs.shape)\n",
        "\n",
        "    imgs = imgs.astype(np.float32) / 255.\n",
        "\n",
        "    for i in range(3):\n",
        "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
        "        means.append(np.mean(pixels))\n",
        "        stdevs.append(np.std(pixels))\n",
        "\n",
        "    means.reverse()  # BGR --> RGB\n",
        "    stdevs.reverse()\n",
        "\n",
        "    print(\"normMean = {}\".format(means))\n",
        "    print(\"normStd = {}\".format(stdevs))\n",
        "    return means,stdevs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rFELfmseKxV"
      },
      "source": [
        "# norm_mean,norm_std = compute_img_mean_std(all_image_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM-17oP-pMYs"
      },
      "source": [
        "norm_mean = (0.763035, 0.54564625, 0.5700399)\n",
        "norm_std = (0.1409281, 0.15261264, 0.16997051)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "jfALDyx-eM9t",
        "outputId": "3936bc25-3da1-43be-b3a2-c20cd9469e3d"
      },
      "source": [
        "df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n",
        "df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n",
        "df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n",
        "df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n",
        "df_original.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "      <th>path</th>\n",
              "      <th>cell_type</th>\n",
              "      <th>cell_type_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0027419</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0025030</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0026769</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0025661</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HAM_0001466</td>\n",
              "      <td>ISIC_0031633</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>75.0</td>\n",
              "      <td>male</td>\n",
              "      <td>ear</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     lesion_id      image_id  ...                       cell_type cell_type_idx\n",
              "0  HAM_0000118  ISIC_0027419  ...  Benign keratosis-like lesions              2\n",
              "1  HAM_0000118  ISIC_0025030  ...  Benign keratosis-like lesions              2\n",
              "2  HAM_0002730  ISIC_0026769  ...  Benign keratosis-like lesions              2\n",
              "3  HAM_0002730  ISIC_0025661  ...  Benign keratosis-like lesions              2\n",
              "4  HAM_0001466  ISIC_0031633  ...  Benign keratosis-like lesions              2\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "NXlI1GCKePD9",
        "outputId": "c345643b-ed42-48d6-8e63-5ed85e7019c6"
      },
      "source": [
        "# this will tell us how many images are associated with each lesion_id\n",
        "df_undup = df_original.groupby('lesion_id').count()\n",
        "# now we filter out lesion_id's that have only one image associated with it\n",
        "df_undup = df_undup[df_undup['image_id'] == 1]\n",
        "df_undup.reset_index(inplace=True)\n",
        "df_undup.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "      <th>path</th>\n",
              "      <th>cell_type</th>\n",
              "      <th>cell_type_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAM_0000001</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HAM_0000003</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HAM_0000004</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HAM_0000007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HAM_0000008</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     lesion_id  image_id  dx  ...  path  cell_type  cell_type_idx\n",
              "0  HAM_0000001         1   1  ...     1          1              1\n",
              "1  HAM_0000003         1   1  ...     1          1              1\n",
              "2  HAM_0000004         1   1  ...     1          1              1\n",
              "3  HAM_0000007         1   1  ...     1          1              1\n",
              "4  HAM_0000008         1   1  ...     1          1              1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "F6VdpWrieQ31",
        "outputId": "fd9ab0b3-fce9-4e5e-9c09-4406c2db81fb"
      },
      "source": [
        "# here we identify lesion_id's that have duplicate images and those that have only one image.\n",
        "def get_duplicates(x):\n",
        "    unique_list = list(df_undup['lesion_id'])\n",
        "    if x in unique_list:\n",
        "        return 'unduplicated'\n",
        "    else:\n",
        "        return 'duplicated'\n",
        "\n",
        "# create a new colum that is a copy of the lesion_id column\n",
        "df_original['duplicates'] = df_original['lesion_id']\n",
        "# apply the function to this new column\n",
        "df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n",
        "df_original.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "      <th>path</th>\n",
              "      <th>cell_type</th>\n",
              "      <th>cell_type_idx</th>\n",
              "      <th>duplicates</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0027419</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "      <td>duplicated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0025030</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "      <td>duplicated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0026769</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "      <td>duplicated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0025661</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "      <td>duplicated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HAM_0001466</td>\n",
              "      <td>ISIC_0031633</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>75.0</td>\n",
              "      <td>male</td>\n",
              "      <td>ear</td>\n",
              "      <td>/content/drive/MyDrive/input499/HAM10000_image...</td>\n",
              "      <td>Benign keratosis-like lesions</td>\n",
              "      <td>2</td>\n",
              "      <td>duplicated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     lesion_id      image_id  ... cell_type_idx  duplicates\n",
              "0  HAM_0000118  ISIC_0027419  ...             2  duplicated\n",
              "1  HAM_0000118  ISIC_0025030  ...             2  duplicated\n",
              "2  HAM_0002730  ISIC_0026769  ...             2  duplicated\n",
              "3  HAM_0002730  ISIC_0025661  ...             2  duplicated\n",
              "4  HAM_0001466  ISIC_0031633  ...             2  duplicated\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZYpftUqeTVm",
        "outputId": "35b73351-1020-4008-82ff-73f49b673717"
      },
      "source": [
        "df_original['duplicates'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "unduplicated    5514\n",
              "duplicated      4501\n",
              "Name: duplicates, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H5V4KBVeVrX",
        "outputId": "5fb0149c-2e86-449f-89fb-44a89ca69ba5"
      },
      "source": [
        "# now we filter out images that don't have duplicates\n",
        "df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n",
        "df_undup.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5514, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHBC7w_heX5F",
        "outputId": "b28837a9-d659-402e-e226-990caa12faaf"
      },
      "source": [
        "# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\n",
        "y = df_undup['cell_type_idx']\n",
        "_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n",
        "df_val.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1103, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V4kl9kleZq1",
        "outputId": "3ce56aaa-16a8-4a62-9049-c2c56880d6d9"
      },
      "source": [
        "df_val['cell_type_idx'].value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    883\n",
              "2     88\n",
              "5     46\n",
              "1     35\n",
              "0     30\n",
              "6     13\n",
              "3      8\n",
              "Name: cell_type_idx, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SerSS8megQ1",
        "outputId": "2ee9610e-4189-4a1a-a6f1-6db1229e4964"
      },
      "source": [
        "# This set will be df_original excluding all rows that are in the val set\n",
        "# This function identifies if an image is part of the train or val set.\n",
        "def get_val_rows(x):\n",
        "    # create a list of all the lesion_id's in the val set\n",
        "    val_list = list(df_val['image_id'])\n",
        "    if str(x) in val_list:\n",
        "        return 'val'\n",
        "    else:\n",
        "        return 'train'\n",
        "\n",
        "# identify train and val rows\n",
        "# create a new colum that is a copy of the image_id column\n",
        "df_original['train_or_val'] = df_original['image_id']\n",
        "# apply the function to this new column\n",
        "df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n",
        "# filter out train rows\n",
        "df_train = df_original[df_original['train_or_val'] == 'train']\n",
        "print(len(df_train))\n",
        "print(len(df_val))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8912\n",
            "1103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzvikaQueift",
        "outputId": "47e2d78e-da76-4638-cd4c-e6fe02119cad"
      },
      "source": [
        "df_train['cell_type_idx'].value_counts()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    5822\n",
              "5    1067\n",
              "2    1011\n",
              "1     479\n",
              "0     297\n",
              "6     129\n",
              "3     107\n",
              "Name: cell_type_idx, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9pAqOLHekZV",
        "outputId": "2408f7d4-59f2-495f-9a13-c6cda9a28d1b"
      },
      "source": [
        "df_val['cell_type'].value_counts()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Melanocytic nevi                  883\n",
              "Benign keratosis-like lesions      88\n",
              "Melanoma                           46\n",
              "Basal cell carcinoma               35\n",
              "Actinic keratoses                  30\n",
              "Vascular lesions                   13\n",
              "Dermatofibroma                      8\n",
              "Name: cell_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to63iOmbel-u",
        "outputId": "dde634fa-ed5b-457a-aa11-5f02c30e2ae7"
      },
      "source": [
        "# Copy fewer class to balance the number of 7 classes\n",
        "data_aug_rate = [15,10,5,50,0,40,5]\n",
        "for i in range(7):\n",
        "    if data_aug_rate[i]:\n",
        "        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
        "df_train['cell_type'].value_counts()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Melanoma                          42680\n",
              "Melanocytic nevi                   5822\n",
              "Dermatofibroma                     5350\n",
              "Benign keratosis-like lesions      5055\n",
              "Basal cell carcinoma               4790\n",
              "Actinic keratoses                  4455\n",
              "Vascular lesions                    645\n",
              "Name: cell_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gXIGKqden9f"
      },
      "source": [
        "# # We can split the test set again in a validation set and a true test set:\n",
        "# df_val, df_test = train_test_split(df_val, test_size=0.5)\n",
        "df_train = df_train.reset_index()\n",
        "df_val = df_val.reset_index()\n",
        "# df_test = df_test.reset_index()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtAPW2Ovi9rf"
      },
      "source": [
        "def save_checkpoint(state, filename = \"checkpoints.pth.tar\"):\n",
        "  print(\"======> Saving Checkpoint.\")\n",
        "  torch.save(state, filename)\n",
        "  print(\"******************************************************\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtTUVJK-jxkm"
      },
      "source": [
        "def load_checkpoint(checkpoint):\n",
        "  print(\"=====> Loading Checkpoint.\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  epoch.load_state_dict(checkpoint['epoch'])\n",
        "  loss_val.load_state_dict(checkpoint['loss_val'])\n",
        "  acc_val.load_state_dict(checkpoint['acc_val'])\n",
        "  loss_train.load_state_dict(checkpoint['loss_train'])\n",
        "  acc_train.load_state_dict(checkpoint['acc_train'])\n",
        "  best_val_acc.load_state_dict(checkpoint['best_val_acc'])\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzuFnwDImTX"
      },
      "source": [
        "**MODEL BUILDING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuI1H-DoeqA9"
      },
      "source": [
        "# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n",
        "# If feature_extract = False, the model is finetuned and all model parameters are updated. \n",
        "# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgyANTmnerwV"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18, resnet34, resnet50, resnet101\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet121\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    return model_ft, input_size"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9a98a14720564657a7367f6c245fa14a",
            "603a96e19fdf4de7910d15ea456324f0",
            "7ba3460dc3694c6091eb09ee73bce284",
            "a5150a77ac5a4c3aadf01d6d8a590be5",
            "41fbbabe6770400e8b4a5d667d399c37",
            "30bb28c85e6d456a9256c0b55517312a",
            "e79f27608ca84c79b75a41ad255e36b3",
            "2907c43af34c4836937d3bcb3316d4f5"
          ]
        },
        "id": "X-AUAsPzeucF",
        "outputId": "155cf54f-62a5-479b-c889-b57e7bf40f68"
      },
      "source": [
        "# resnet,vgg,densenet,inception\n",
        "model_name = 'vgg'\n",
        "num_classes = 7\n",
        "feature_extract = False\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "# Define the device:\n",
        "device = torch.device('cuda:0')\n",
        "# Put the model on the device:\n",
        "model = model_ft.to(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a98a14720564657a7367f6c245fa14a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=531503671.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA2v6p-9ewIQ"
      },
      "source": [
        "# norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
        "# norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
        "# define the transformation of the train images.\n",
        "train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n",
        "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
        "                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n",
        "# define the transformation of the val images.\n",
        "val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n",
        "                                    transforms.Normalize(norm_mean, norm_std)])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO6hKxMEeyfV"
      },
      "source": [
        "# Define a pytorch dataloader for this dataset\n",
        "class HAM10000(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        X = Image.open(self.df['path'][index])\n",
        "        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Nhwnuje0z_"
      },
      "source": [
        "# Define the training set using the table train_df and using our defined transitions (train_transform)\n",
        "training_set = HAM10000(df_train, transform=train_transform)\n",
        "train_loader = DataLoader(training_set, batch_size=64, shuffle=True, num_workers=4)\n",
        "# Same for the validation set:\n",
        "validation_set = HAM10000(df_val, transform=train_transform)\n",
        "val_loader = DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLTbU6XQe2oW"
      },
      "source": [
        "# we use Adam optimizer, use cross entropy loss as our loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7JpuA0nIsmd"
      },
      "source": [
        "**MODEL TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz0DUP8se5HV"
      },
      "source": [
        "# this function is used during training process, to calculation the loss and accuracy\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMRYblHRe7gX"
      },
      "source": [
        "total_loss_train, total_acc_train = [],[]\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = AverageMeter()\n",
        "    train_acc = AverageMeter()\n",
        "    curr_iter = (epoch - 1) * len(train_loader)\n",
        "    for i, data in enumerate(train_loader):\n",
        "        images, labels = data\n",
        "        N = images.size(0)\n",
        "        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
        "        images = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        prediction = outputs.max(1, keepdim=True)[1]\n",
        "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
        "        train_loss.update(loss.item())\n",
        "        curr_iter += 1\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
        "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
        "            total_loss_train.append(train_loss.avg)\n",
        "            total_acc_train.append(train_acc.avg)\n",
        "    return train_loss.avg, train_acc.avg"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ZgYEQOe9_R"
      },
      "source": [
        "def validate(val_loader, model, criterion, optimizer, epoch):\n",
        "    model.eval()\n",
        "    val_loss = AverageMeter()\n",
        "    val_acc = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            images, labels = data\n",
        "            N = images.size(0)\n",
        "            images = Variable(images).to(device)\n",
        "            labels = Variable(labels).to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            prediction = outputs.max(1, keepdim=True)[1]\n",
        "\n",
        "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
        "\n",
        "            val_loss.update(criterion(outputs, labels).item())\n",
        "\n",
        "    print('------------------------------------------------------------')\n",
        "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
        "    print('------------------------------------------------------------')\n",
        "    return val_loss.avg, val_acc.avg"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbZ9NYfpRD0a"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahs1o0GUfAp1",
        "outputId": "b0f3bec6-6cac-4163-9560-c368d73e5a08"
      },
      "source": [
        "epoch_num = 25\n",
        "load_model = True\n",
        "best_val_acc = 0\n",
        "total_loss_val, total_acc_val = [],[]\n",
        "\n",
        "\n",
        "for epoch in range(1, epoch_num+1):\n",
        "\n",
        "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n",
        "    total_loss_val.append(loss_val)\n",
        "    total_acc_val.append(acc_val)\n",
        "    if acc_val > best_val_acc:\n",
        "        best_val_acc = acc_val\n",
        "        print('*****************************************************')\n",
        "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
        "        print('*****************************************************')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 1], [iter 100 / 1075], [train loss 0.90483], [train acc 0.70344]\n",
            "[epoch 1], [iter 200 / 1075], [train loss 0.73146], [train acc 0.75898]\n",
            "[epoch 1], [iter 300 / 1075], [train loss 0.64082], [train acc 0.78849]\n",
            "[epoch 1], [iter 400 / 1075], [train loss 0.57454], [train acc 0.81133]\n",
            "[epoch 1], [iter 500 / 1075], [train loss 0.52766], [train acc 0.82725]\n",
            "[epoch 1], [iter 600 / 1075], [train loss 0.48999], [train acc 0.84034]\n",
            "[epoch 1], [iter 700 / 1075], [train loss 0.46136], [train acc 0.85038]\n",
            "[epoch 1], [iter 800 / 1075], [train loss 0.43538], [train acc 0.85916]\n",
            "[epoch 1], [iter 900 / 1075], [train loss 0.41305], [train acc 0.86620]\n",
            "[epoch 1], [iter 1000 / 1075], [train loss 0.39338], [train acc 0.87252]\n",
            "------------------------------------------------------------\n",
            "[epoch 1], [val loss 0.58966], [val acc 0.79965]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 1], [val loss 0.58966], [val acc 0.79965]\n",
            "*****************************************************\n",
            "[epoch 2], [iter 100 / 1075], [train loss 0.16315], [train acc 0.94609]\n",
            "[epoch 2], [iter 200 / 1075], [train loss 0.16489], [train acc 0.94602]\n",
            "[epoch 2], [iter 300 / 1075], [train loss 0.16830], [train acc 0.94495]\n",
            "[epoch 2], [iter 400 / 1075], [train loss 0.16211], [train acc 0.94688]\n",
            "[epoch 2], [iter 500 / 1075], [train loss 0.16106], [train acc 0.94716]\n",
            "[epoch 2], [iter 600 / 1075], [train loss 0.15866], [train acc 0.94799]\n",
            "[epoch 2], [iter 700 / 1075], [train loss 0.15740], [train acc 0.94862]\n",
            "[epoch 2], [iter 800 / 1075], [train loss 0.15577], [train acc 0.94906]\n",
            "[epoch 2], [iter 900 / 1075], [train loss 0.15234], [train acc 0.95028]\n",
            "[epoch 2], [iter 1000 / 1075], [train loss 0.14858], [train acc 0.95152]\n",
            "------------------------------------------------------------\n",
            "[epoch 2], [val loss 0.40365], [val acc 0.88061]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 2], [val loss 0.40365], [val acc 0.88061]\n",
            "*****************************************************\n",
            "[epoch 3], [iter 100 / 1075], [train loss 0.11870], [train acc 0.96297]\n",
            "[epoch 3], [iter 200 / 1075], [train loss 0.10903], [train acc 0.96672]\n",
            "[epoch 3], [iter 300 / 1075], [train loss 0.10493], [train acc 0.96797]\n",
            "[epoch 3], [iter 400 / 1075], [train loss 0.10188], [train acc 0.96848]\n",
            "[epoch 3], [iter 500 / 1075], [train loss 0.10169], [train acc 0.96822]\n",
            "[epoch 3], [iter 600 / 1075], [train loss 0.09940], [train acc 0.96862]\n",
            "[epoch 3], [iter 700 / 1075], [train loss 0.09976], [train acc 0.96810]\n",
            "[epoch 3], [iter 800 / 1075], [train loss 0.09801], [train acc 0.96859]\n",
            "[epoch 3], [iter 900 / 1075], [train loss 0.09780], [train acc 0.96854]\n",
            "[epoch 3], [iter 1000 / 1075], [train loss 0.09684], [train acc 0.96878]\n",
            "------------------------------------------------------------\n",
            "[epoch 3], [val loss 0.35205], [val acc 0.89016]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 3], [val loss 0.35205], [val acc 0.89016]\n",
            "*****************************************************\n",
            "[epoch 4], [iter 100 / 1075], [train loss 0.07514], [train acc 0.97781]\n",
            "[epoch 4], [iter 200 / 1075], [train loss 0.07160], [train acc 0.97797]\n",
            "[epoch 4], [iter 300 / 1075], [train loss 0.07613], [train acc 0.97573]\n",
            "[epoch 4], [iter 400 / 1075], [train loss 0.07597], [train acc 0.97621]\n",
            "[epoch 4], [iter 500 / 1075], [train loss 0.07723], [train acc 0.97584]\n",
            "[epoch 4], [iter 600 / 1075], [train loss 0.07550], [train acc 0.97628]\n",
            "[epoch 4], [iter 700 / 1075], [train loss 0.07446], [train acc 0.97627]\n",
            "[epoch 4], [iter 800 / 1075], [train loss 0.07314], [train acc 0.97660]\n",
            "[epoch 4], [iter 900 / 1075], [train loss 0.07261], [train acc 0.97658]\n",
            "[epoch 4], [iter 1000 / 1075], [train loss 0.07226], [train acc 0.97647]\n",
            "------------------------------------------------------------\n",
            "[epoch 4], [val loss 0.38422], [val acc 0.89774]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 4], [val loss 0.38422], [val acc 0.89774]\n",
            "*****************************************************\n",
            "[epoch 5], [iter 100 / 1075], [train loss 0.05725], [train acc 0.98094]\n",
            "[epoch 5], [iter 200 / 1075], [train loss 0.06077], [train acc 0.98047]\n",
            "[epoch 5], [iter 300 / 1075], [train loss 0.05940], [train acc 0.98109]\n",
            "[epoch 5], [iter 400 / 1075], [train loss 0.05983], [train acc 0.98090]\n",
            "[epoch 5], [iter 500 / 1075], [train loss 0.06150], [train acc 0.98056]\n",
            "[epoch 5], [iter 600 / 1075], [train loss 0.06169], [train acc 0.98029]\n",
            "[epoch 5], [iter 700 / 1075], [train loss 0.06279], [train acc 0.97991]\n",
            "[epoch 5], [iter 800 / 1075], [train loss 0.06139], [train acc 0.98043]\n",
            "[epoch 5], [iter 900 / 1075], [train loss 0.06083], [train acc 0.98059]\n",
            "[epoch 5], [iter 1000 / 1075], [train loss 0.06009], [train acc 0.98084]\n",
            "------------------------------------------------------------\n",
            "[epoch 5], [val loss 0.49422], [val acc 0.88148]\n",
            "------------------------------------------------------------\n",
            "[epoch 6], [iter 100 / 1075], [train loss 0.05515], [train acc 0.98203]\n",
            "[epoch 6], [iter 200 / 1075], [train loss 0.05440], [train acc 0.98227]\n",
            "[epoch 6], [iter 300 / 1075], [train loss 0.05146], [train acc 0.98333]\n",
            "[epoch 6], [iter 400 / 1075], [train loss 0.04883], [train acc 0.98395]\n",
            "[epoch 6], [iter 500 / 1075], [train loss 0.04915], [train acc 0.98350]\n",
            "[epoch 6], [iter 600 / 1075], [train loss 0.04939], [train acc 0.98320]\n",
            "[epoch 6], [iter 700 / 1075], [train loss 0.05034], [train acc 0.98321]\n",
            "[epoch 6], [iter 800 / 1075], [train loss 0.05039], [train acc 0.98342]\n",
            "[epoch 6], [iter 900 / 1075], [train loss 0.05036], [train acc 0.98344]\n",
            "[epoch 6], [iter 1000 / 1075], [train loss 0.05067], [train acc 0.98334]\n",
            "------------------------------------------------------------\n",
            "[epoch 6], [val loss 0.49833], [val acc 0.88779]\n",
            "------------------------------------------------------------\n",
            "[epoch 7], [iter 100 / 1075], [train loss 0.04176], [train acc 0.98516]\n",
            "[epoch 7], [iter 200 / 1075], [train loss 0.04215], [train acc 0.98664]\n",
            "[epoch 7], [iter 300 / 1075], [train loss 0.03908], [train acc 0.98750]\n",
            "[epoch 7], [iter 400 / 1075], [train loss 0.04136], [train acc 0.98687]\n",
            "[epoch 7], [iter 500 / 1075], [train loss 0.04343], [train acc 0.98631]\n",
            "[epoch 7], [iter 600 / 1075], [train loss 0.04404], [train acc 0.98615]\n",
            "[epoch 7], [iter 700 / 1075], [train loss 0.04385], [train acc 0.98625]\n",
            "[epoch 7], [iter 800 / 1075], [train loss 0.04491], [train acc 0.98588]\n",
            "[epoch 7], [iter 900 / 1075], [train loss 0.04535], [train acc 0.98595]\n",
            "[epoch 7], [iter 1000 / 1075], [train loss 0.04518], [train acc 0.98573]\n",
            "------------------------------------------------------------\n",
            "[epoch 7], [val loss 0.37393], [val acc 0.90318]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 7], [val loss 0.37393], [val acc 0.90318]\n",
            "*****************************************************\n",
            "[epoch 8], [iter 100 / 1075], [train loss 0.04121], [train acc 0.98625]\n",
            "[epoch 8], [iter 200 / 1075], [train loss 0.03848], [train acc 0.98789]\n",
            "[epoch 8], [iter 300 / 1075], [train loss 0.03794], [train acc 0.98818]\n",
            "[epoch 8], [iter 400 / 1075], [train loss 0.03549], [train acc 0.98879]\n",
            "[epoch 8], [iter 500 / 1075], [train loss 0.03511], [train acc 0.98881]\n",
            "[epoch 8], [iter 600 / 1075], [train loss 0.03640], [train acc 0.98867]\n",
            "[epoch 8], [iter 700 / 1075], [train loss 0.03669], [train acc 0.98830]\n",
            "[epoch 8], [iter 800 / 1075], [train loss 0.03809], [train acc 0.98781]\n",
            "[epoch 8], [iter 900 / 1075], [train loss 0.03899], [train acc 0.98774]\n",
            "[epoch 8], [iter 1000 / 1075], [train loss 0.03842], [train acc 0.98784]\n",
            "------------------------------------------------------------\n",
            "[epoch 8], [val loss 0.44000], [val acc 0.90862]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 8], [val loss 0.44000], [val acc 0.90862]\n",
            "*****************************************************\n",
            "[epoch 9], [iter 100 / 1075], [train loss 0.02913], [train acc 0.98859]\n",
            "[epoch 9], [iter 200 / 1075], [train loss 0.03173], [train acc 0.98883]\n",
            "[epoch 9], [iter 300 / 1075], [train loss 0.03218], [train acc 0.98922]\n",
            "[epoch 9], [iter 400 / 1075], [train loss 0.03284], [train acc 0.98887]\n",
            "[epoch 9], [iter 500 / 1075], [train loss 0.03294], [train acc 0.98887]\n",
            "[epoch 9], [iter 600 / 1075], [train loss 0.03448], [train acc 0.98854]\n",
            "[epoch 9], [iter 700 / 1075], [train loss 0.03475], [train acc 0.98877]\n",
            "[epoch 9], [iter 800 / 1075], [train loss 0.03528], [train acc 0.98863]\n",
            "[epoch 9], [iter 900 / 1075], [train loss 0.03545], [train acc 0.98835]\n",
            "[epoch 9], [iter 1000 / 1075], [train loss 0.03420], [train acc 0.98867]\n",
            "------------------------------------------------------------\n",
            "[epoch 9], [val loss 0.46008], [val acc 0.91383]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 9], [val loss 0.46008], [val acc 0.91383]\n",
            "*****************************************************\n",
            "[epoch 10], [iter 100 / 1075], [train loss 0.03835], [train acc 0.98672]\n",
            "[epoch 10], [iter 200 / 1075], [train loss 0.03301], [train acc 0.98867]\n",
            "[epoch 10], [iter 300 / 1075], [train loss 0.03081], [train acc 0.98969]\n",
            "[epoch 10], [iter 400 / 1075], [train loss 0.03201], [train acc 0.98949]\n",
            "[epoch 10], [iter 500 / 1075], [train loss 0.03152], [train acc 0.98944]\n",
            "[epoch 10], [iter 600 / 1075], [train loss 0.03147], [train acc 0.98948]\n",
            "[epoch 10], [iter 700 / 1075], [train loss 0.03231], [train acc 0.98917]\n",
            "[epoch 10], [iter 800 / 1075], [train loss 0.03193], [train acc 0.98943]\n",
            "[epoch 10], [iter 900 / 1075], [train loss 0.03225], [train acc 0.98950]\n",
            "[epoch 10], [iter 1000 / 1075], [train loss 0.03210], [train acc 0.98958]\n",
            "------------------------------------------------------------\n",
            "[epoch 10], [val loss 0.45983], [val acc 0.89734]\n",
            "------------------------------------------------------------\n",
            "[epoch 11], [iter 100 / 1075], [train loss 0.03376], [train acc 0.98875]\n",
            "[epoch 11], [iter 200 / 1075], [train loss 0.03195], [train acc 0.98953]\n",
            "[epoch 11], [iter 300 / 1075], [train loss 0.03195], [train acc 0.98984]\n",
            "[epoch 11], [iter 400 / 1075], [train loss 0.03126], [train acc 0.99008]\n",
            "[epoch 11], [iter 500 / 1075], [train loss 0.03122], [train acc 0.99003]\n",
            "[epoch 11], [iter 600 / 1075], [train loss 0.03118], [train acc 0.99008]\n",
            "[epoch 11], [iter 700 / 1075], [train loss 0.03165], [train acc 0.98980]\n",
            "[epoch 11], [iter 800 / 1075], [train loss 0.03130], [train acc 0.98982]\n",
            "[epoch 11], [iter 900 / 1075], [train loss 0.03069], [train acc 0.99014]\n",
            "[epoch 11], [iter 1000 / 1075], [train loss 0.03026], [train acc 0.99028]\n",
            "------------------------------------------------------------\n",
            "[epoch 11], [val loss 0.47315], [val acc 0.90862]\n",
            "------------------------------------------------------------\n",
            "[epoch 12], [iter 100 / 1075], [train loss 0.03021], [train acc 0.99094]\n",
            "[epoch 12], [iter 200 / 1075], [train loss 0.03303], [train acc 0.99016]\n",
            "[epoch 12], [iter 300 / 1075], [train loss 0.03407], [train acc 0.98984]\n",
            "[epoch 12], [iter 400 / 1075], [train loss 0.03073], [train acc 0.99066]\n",
            "[epoch 12], [iter 500 / 1075], [train loss 0.03053], [train acc 0.99056]\n",
            "[epoch 12], [iter 600 / 1075], [train loss 0.03088], [train acc 0.99039]\n",
            "[epoch 12], [iter 700 / 1075], [train loss 0.03055], [train acc 0.99049]\n",
            "[epoch 12], [iter 800 / 1075], [train loss 0.02954], [train acc 0.99080]\n",
            "[epoch 12], [iter 900 / 1075], [train loss 0.02933], [train acc 0.99078]\n",
            "[epoch 12], [iter 1000 / 1075], [train loss 0.02836], [train acc 0.99089]\n",
            "------------------------------------------------------------\n",
            "[epoch 12], [val loss 0.57389], [val acc 0.89190]\n",
            "------------------------------------------------------------\n",
            "[epoch 13], [iter 100 / 1075], [train loss 0.02134], [train acc 0.99328]\n",
            "[epoch 13], [iter 200 / 1075], [train loss 0.02776], [train acc 0.99172]\n",
            "[epoch 13], [iter 300 / 1075], [train loss 0.02383], [train acc 0.99255]\n",
            "[epoch 13], [iter 400 / 1075], [train loss 0.02444], [train acc 0.99223]\n",
            "[epoch 13], [iter 500 / 1075], [train loss 0.02489], [train acc 0.99178]\n",
            "[epoch 13], [iter 600 / 1075], [train loss 0.02472], [train acc 0.99193]\n",
            "[epoch 13], [iter 700 / 1075], [train loss 0.02562], [train acc 0.99183]\n",
            "[epoch 13], [iter 800 / 1075], [train loss 0.02723], [train acc 0.99133]\n",
            "[epoch 13], [iter 900 / 1075], [train loss 0.02785], [train acc 0.99108]\n",
            "[epoch 13], [iter 1000 / 1075], [train loss 0.02782], [train acc 0.99108]\n",
            "------------------------------------------------------------\n",
            "[epoch 13], [val loss 0.46788], [val acc 0.90666]\n",
            "------------------------------------------------------------\n",
            "[epoch 14], [iter 100 / 1075], [train loss 0.02310], [train acc 0.99172]\n",
            "[epoch 14], [iter 200 / 1075], [train loss 0.02839], [train acc 0.99016]\n",
            "[epoch 14], [iter 300 / 1075], [train loss 0.02801], [train acc 0.99016]\n",
            "[epoch 14], [iter 400 / 1075], [train loss 0.02634], [train acc 0.99098]\n",
            "[epoch 14], [iter 500 / 1075], [train loss 0.02579], [train acc 0.99128]\n",
            "[epoch 14], [iter 600 / 1075], [train loss 0.02553], [train acc 0.99128]\n",
            "[epoch 14], [iter 700 / 1075], [train loss 0.02361], [train acc 0.99208]\n",
            "[epoch 14], [iter 800 / 1075], [train loss 0.02307], [train acc 0.99227]\n",
            "[epoch 14], [iter 900 / 1075], [train loss 0.02295], [train acc 0.99238]\n",
            "[epoch 14], [iter 1000 / 1075], [train loss 0.02373], [train acc 0.99230]\n",
            "------------------------------------------------------------\n",
            "[epoch 14], [val loss 0.51436], [val acc 0.90428]\n",
            "------------------------------------------------------------\n",
            "[epoch 15], [iter 100 / 1075], [train loss 0.03472], [train acc 0.98984]\n",
            "[epoch 15], [iter 200 / 1075], [train loss 0.03062], [train acc 0.99062]\n",
            "[epoch 15], [iter 300 / 1075], [train loss 0.02619], [train acc 0.99187]\n",
            "[epoch 15], [iter 400 / 1075], [train loss 0.02432], [train acc 0.99227]\n",
            "[epoch 15], [iter 500 / 1075], [train loss 0.02510], [train acc 0.99225]\n",
            "[epoch 15], [iter 600 / 1075], [train loss 0.02346], [train acc 0.99289]\n",
            "[epoch 15], [iter 700 / 1075], [train loss 0.02165], [train acc 0.99346]\n",
            "[epoch 15], [iter 800 / 1075], [train loss 0.02237], [train acc 0.99316]\n",
            "[epoch 15], [iter 900 / 1075], [train loss 0.02275], [train acc 0.99306]\n",
            "[epoch 15], [iter 1000 / 1075], [train loss 0.02345], [train acc 0.99286]\n",
            "------------------------------------------------------------\n",
            "[epoch 15], [val loss 0.48910], [val acc 0.89300]\n",
            "------------------------------------------------------------\n",
            "[epoch 16], [iter 100 / 1075], [train loss 0.02039], [train acc 0.99422]\n",
            "[epoch 16], [iter 200 / 1075], [train loss 0.01996], [train acc 0.99453]\n",
            "[epoch 16], [iter 300 / 1075], [train loss 0.01957], [train acc 0.99422]\n",
            "[epoch 16], [iter 400 / 1075], [train loss 0.02210], [train acc 0.99352]\n",
            "[epoch 16], [iter 500 / 1075], [train loss 0.02275], [train acc 0.99322]\n",
            "[epoch 16], [iter 600 / 1075], [train loss 0.02330], [train acc 0.99294]\n",
            "[epoch 16], [iter 700 / 1075], [train loss 0.02275], [train acc 0.99295]\n",
            "[epoch 16], [iter 800 / 1075], [train loss 0.02171], [train acc 0.99314]\n",
            "[epoch 16], [iter 900 / 1075], [train loss 0.02226], [train acc 0.99283]\n",
            "[epoch 16], [iter 1000 / 1075], [train loss 0.02252], [train acc 0.99264]\n",
            "------------------------------------------------------------\n",
            "[epoch 16], [val loss 0.51675], [val acc 0.90341]\n",
            "------------------------------------------------------------\n",
            "[epoch 17], [iter 100 / 1075], [train loss 0.01793], [train acc 0.99516]\n",
            "[epoch 17], [iter 200 / 1075], [train loss 0.01950], [train acc 0.99516]\n",
            "[epoch 17], [iter 300 / 1075], [train loss 0.02342], [train acc 0.99328]\n",
            "[epoch 17], [iter 400 / 1075], [train loss 0.02329], [train acc 0.99297]\n",
            "[epoch 17], [iter 500 / 1075], [train loss 0.02345], [train acc 0.99294]\n",
            "[epoch 17], [iter 600 / 1075], [train loss 0.02385], [train acc 0.99297]\n",
            "[epoch 17], [iter 700 / 1075], [train loss 0.02360], [train acc 0.99304]\n",
            "[epoch 17], [iter 800 / 1075], [train loss 0.02355], [train acc 0.99301]\n",
            "[epoch 17], [iter 900 / 1075], [train loss 0.02263], [train acc 0.99319]\n",
            "[epoch 17], [iter 1000 / 1075], [train loss 0.02321], [train acc 0.99297]\n",
            "------------------------------------------------------------\n",
            "[epoch 17], [val loss 0.46740], [val acc 0.91383]\n",
            "------------------------------------------------------------\n",
            "[epoch 18], [iter 100 / 1075], [train loss 0.01934], [train acc 0.99266]\n",
            "[epoch 18], [iter 200 / 1075], [train loss 0.02059], [train acc 0.99273]\n",
            "[epoch 18], [iter 300 / 1075], [train loss 0.01754], [train acc 0.99370]\n",
            "[epoch 18], [iter 400 / 1075], [train loss 0.01554], [train acc 0.99441]\n",
            "[epoch 18], [iter 500 / 1075], [train loss 0.01738], [train acc 0.99397]\n",
            "[epoch 18], [iter 600 / 1075], [train loss 0.01892], [train acc 0.99365]\n",
            "[epoch 18], [iter 700 / 1075], [train loss 0.01990], [train acc 0.99333]\n",
            "[epoch 18], [iter 800 / 1075], [train loss 0.02099], [train acc 0.99303]\n",
            "[epoch 18], [iter 900 / 1075], [train loss 0.02009], [train acc 0.99325]\n",
            "[epoch 18], [iter 1000 / 1075], [train loss 0.01957], [train acc 0.99347]\n",
            "------------------------------------------------------------\n",
            "[epoch 18], [val loss 0.55052], [val acc 0.90081]\n",
            "------------------------------------------------------------\n",
            "[epoch 19], [iter 100 / 1075], [train loss 0.02720], [train acc 0.99266]\n",
            "[epoch 19], [iter 200 / 1075], [train loss 0.02183], [train acc 0.99328]\n",
            "[epoch 19], [iter 300 / 1075], [train loss 0.01932], [train acc 0.99370]\n",
            "[epoch 19], [iter 400 / 1075], [train loss 0.02143], [train acc 0.99332]\n",
            "[epoch 19], [iter 500 / 1075], [train loss 0.02151], [train acc 0.99331]\n",
            "[epoch 19], [iter 600 / 1075], [train loss 0.02135], [train acc 0.99336]\n",
            "[epoch 19], [iter 700 / 1075], [train loss 0.02180], [train acc 0.99337]\n",
            "[epoch 19], [iter 800 / 1075], [train loss 0.02204], [train acc 0.99334]\n",
            "[epoch 19], [iter 900 / 1075], [train loss 0.02206], [train acc 0.99326]\n",
            "[epoch 19], [iter 1000 / 1075], [train loss 0.02204], [train acc 0.99316]\n",
            "------------------------------------------------------------\n",
            "[epoch 19], [val loss 0.42667], [val acc 0.91817]\n",
            "------------------------------------------------------------\n",
            "*****************************************************\n",
            "best record: [epoch 19], [val loss 0.42667], [val acc 0.91817]\n",
            "*****************************************************\n",
            "[epoch 20], [iter 100 / 1075], [train loss 0.01568], [train acc 0.99406]\n",
            "[epoch 20], [iter 200 / 1075], [train loss 0.01288], [train acc 0.99523]\n",
            "[epoch 20], [iter 300 / 1075], [train loss 0.01274], [train acc 0.99542]\n",
            "[epoch 20], [iter 400 / 1075], [train loss 0.01373], [train acc 0.99535]\n",
            "[epoch 20], [iter 500 / 1075], [train loss 0.01449], [train acc 0.99522]\n",
            "[epoch 20], [iter 600 / 1075], [train loss 0.01540], [train acc 0.99500]\n",
            "[epoch 20], [iter 700 / 1075], [train loss 0.01746], [train acc 0.99435]\n",
            "[epoch 20], [iter 800 / 1075], [train loss 0.01768], [train acc 0.99432]\n",
            "[epoch 20], [iter 900 / 1075], [train loss 0.01877], [train acc 0.99408]\n",
            "[epoch 20], [iter 1000 / 1075], [train loss 0.01774], [train acc 0.99433]\n",
            "------------------------------------------------------------\n",
            "[epoch 20], [val loss 0.60518], [val acc 0.91644]\n",
            "------------------------------------------------------------\n",
            "[epoch 21], [iter 100 / 1075], [train loss 0.01154], [train acc 0.99625]\n",
            "[epoch 21], [iter 200 / 1075], [train loss 0.01511], [train acc 0.99547]\n",
            "[epoch 21], [iter 300 / 1075], [train loss 0.01422], [train acc 0.99578]\n",
            "[epoch 21], [iter 400 / 1075], [train loss 0.01465], [train acc 0.99555]\n",
            "[epoch 21], [iter 500 / 1075], [train loss 0.01552], [train acc 0.99525]\n",
            "[epoch 21], [iter 600 / 1075], [train loss 0.01622], [train acc 0.99503]\n",
            "[epoch 21], [iter 700 / 1075], [train loss 0.01661], [train acc 0.99480]\n",
            "[epoch 21], [iter 800 / 1075], [train loss 0.01714], [train acc 0.99461]\n",
            "[epoch 21], [iter 900 / 1075], [train loss 0.01703], [train acc 0.99460]\n",
            "[epoch 21], [iter 1000 / 1075], [train loss 0.01695], [train acc 0.99461]\n",
            "------------------------------------------------------------\n",
            "[epoch 21], [val loss 0.68919], [val acc 0.89994]\n",
            "------------------------------------------------------------\n",
            "[epoch 22], [iter 100 / 1075], [train loss 0.02751], [train acc 0.99078]\n",
            "[epoch 22], [iter 200 / 1075], [train loss 0.02275], [train acc 0.99234]\n",
            "[epoch 22], [iter 300 / 1075], [train loss 0.02021], [train acc 0.99328]\n",
            "[epoch 22], [iter 400 / 1075], [train loss 0.01932], [train acc 0.99348]\n",
            "[epoch 22], [iter 500 / 1075], [train loss 0.01834], [train acc 0.99375]\n",
            "[epoch 22], [iter 600 / 1075], [train loss 0.01942], [train acc 0.99375]\n",
            "[epoch 22], [iter 700 / 1075], [train loss 0.01991], [train acc 0.99388]\n",
            "[epoch 22], [iter 800 / 1075], [train loss 0.01884], [train acc 0.99412]\n",
            "[epoch 22], [iter 900 / 1075], [train loss 0.01930], [train acc 0.99399]\n",
            "[epoch 22], [iter 1000 / 1075], [train loss 0.01838], [train acc 0.99430]\n",
            "------------------------------------------------------------\n",
            "[epoch 22], [val loss 0.54305], [val acc 0.91470]\n",
            "------------------------------------------------------------\n",
            "[epoch 23], [iter 100 / 1075], [train loss 0.01529], [train acc 0.99516]\n",
            "[epoch 23], [iter 200 / 1075], [train loss 0.01349], [train acc 0.99523]\n",
            "[epoch 23], [iter 300 / 1075], [train loss 0.01296], [train acc 0.99547]\n",
            "[epoch 23], [iter 400 / 1075], [train loss 0.01521], [train acc 0.99492]\n",
            "[epoch 23], [iter 500 / 1075], [train loss 0.01514], [train acc 0.99500]\n",
            "[epoch 23], [iter 600 / 1075], [train loss 0.01459], [train acc 0.99526]\n",
            "[epoch 23], [iter 700 / 1075], [train loss 0.01591], [train acc 0.99498]\n",
            "[epoch 23], [iter 800 / 1075], [train loss 0.01611], [train acc 0.99498]\n",
            "[epoch 23], [iter 900 / 1075], [train loss 0.01659], [train acc 0.99495]\n",
            "[epoch 23], [iter 1000 / 1075], [train loss 0.01714], [train acc 0.99491]\n",
            "------------------------------------------------------------\n",
            "[epoch 23], [val loss 0.50982], [val acc 0.91209]\n",
            "------------------------------------------------------------\n",
            "[epoch 24], [iter 100 / 1075], [train loss 0.01257], [train acc 0.99625]\n",
            "[epoch 24], [iter 200 / 1075], [train loss 0.01251], [train acc 0.99609]\n",
            "[epoch 24], [iter 300 / 1075], [train loss 0.01236], [train acc 0.99620]\n",
            "[epoch 24], [iter 400 / 1075], [train loss 0.01502], [train acc 0.99574]\n",
            "[epoch 24], [iter 500 / 1075], [train loss 0.01493], [train acc 0.99584]\n",
            "[epoch 24], [iter 600 / 1075], [train loss 0.01468], [train acc 0.99573]\n",
            "[epoch 24], [iter 700 / 1075], [train loss 0.01543], [train acc 0.99558]\n",
            "[epoch 24], [iter 800 / 1075], [train loss 0.01548], [train acc 0.99555]\n",
            "[epoch 24], [iter 900 / 1075], [train loss 0.01521], [train acc 0.99564]\n",
            "[epoch 24], [iter 1000 / 1075], [train loss 0.01496], [train acc 0.99570]\n",
            "------------------------------------------------------------\n",
            "[epoch 24], [val loss 0.59997], [val acc 0.90775]\n",
            "------------------------------------------------------------\n",
            "[epoch 25], [iter 100 / 1075], [train loss 0.02419], [train acc 0.99266]\n",
            "[epoch 25], [iter 200 / 1075], [train loss 0.01762], [train acc 0.99453]\n",
            "[epoch 25], [iter 300 / 1075], [train loss 0.01843], [train acc 0.99432]\n",
            "[epoch 25], [iter 400 / 1075], [train loss 0.01854], [train acc 0.99434]\n",
            "[epoch 25], [iter 500 / 1075], [train loss 0.01997], [train acc 0.99406]\n",
            "[epoch 25], [iter 600 / 1075], [train loss 0.01872], [train acc 0.99448]\n",
            "[epoch 25], [iter 700 / 1075], [train loss 0.01894], [train acc 0.99444]\n",
            "[epoch 25], [iter 800 / 1075], [train loss 0.02068], [train acc 0.99395]\n",
            "[epoch 25], [iter 900 / 1075], [train loss 0.02080], [train acc 0.99391]\n",
            "[epoch 25], [iter 1000 / 1075], [train loss 0.02110], [train acc 0.99378]\n",
            "------------------------------------------------------------\n",
            "[epoch 25], [val loss 0.51874], [val acc 0.90949]\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrO4-EX8P1Ni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "54b06633-3479-4cb6-918f-259b332690a3"
      },
      "source": [
        "torch.save(model, '/content/drive/MyDrive/input/modelbestnow.h5')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-549f6ef9c791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/input/modelbestnow.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/input/modelbestnow.h5'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cal3lJhEFmP3"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = 'REPLACE_WITH_YOUR_FILE_ID'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRhUmP_II0uy"
      },
      "source": [
        "**MODEL EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83sYS6z-fCsu"
      },
      "source": [
        "fig = plt.figure(num = 2)\n",
        "fig1 = fig.add_subplot(2,1,1)\n",
        "fig2 = fig.add_subplot(2,1,2)\n",
        "fig1.plot(total_loss_train, label = 'training loss')\n",
        "fig1.plot(total_acc_train, label = 'training accuracy')\n",
        "fig2.plot(total_loss_val, label = 'validation loss')\n",
        "fig2.plot(total_acc_val, label = 'validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjiE0c7XfFsV"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=True,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChuVT_fyfHfi"
      },
      "source": [
        "model.eval()\n",
        "y_label = []\n",
        "y_predict = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(val_loader):\n",
        "        images, labels = data\n",
        "        N = images.size(0)\n",
        "        images = Variable(images).to(device)\n",
        "        outputs = model(images)\n",
        "        prediction = outputs.max(1, keepdim=True)[1]\n",
        "        y_label.extend(labels.cpu().numpy())\n",
        "        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n",
        "\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(y_label, y_predict)\n",
        "# plot the confusion matrix\n",
        "plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n",
        "plot_confusion_matrix(confusion_mtx, plot_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7QZHcL3fJ9t"
      },
      "source": [
        "# Generate a classification report\n",
        "report = classification_report(y_label, y_predict, target_names=plot_labels)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIf0A6DfMP3"
      },
      "source": [
        "label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n",
        "plt.bar(np.arange(7),label_frac_error)\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Fraction classified incorrectly')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}